{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re \n",
    "from random import * \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 5])\n",
      "torch.Size([2, 3, 4, 5])\n",
      "Einsum shape:  torch.Size([2, 3, 4, 6])\n",
      "torch.Size([2, 3, 4, 1, 5])\n",
      "torch.Size([1, 1, 4, 6, 5])\n",
      "after mul:  torch.Size([2, 3, 4, 6, 5])\n",
      "after sum:  torch.Size([2, 3, 4, 6])\n",
      "torch.Size([2, 3, 4, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.Tensor([[1,2,3], [1,2,3], [1,2,3]])\n",
    "\n",
    "b = 2\n",
    "h = 3\n",
    "l = 4\n",
    "d = 5\n",
    "r = 6 \n",
    "\n",
    "x = torch.rand(b*h*l*d).view((b,h,l,d))\n",
    "y = torch.rand(l*r*d).view((l,r,d))\n",
    "print(y.size())\n",
    "print(x.size())\n",
    "# print(x)\n",
    "ein_z = torch.einsum(\"bhld, lrd -> bhlr\", x, y)\n",
    "print(\"Einsum shape: \", ein_z.size())\n",
    "# print(x.transpose(0,1))\n",
    "\n",
    "x_reshaped = x.unsqueeze(3)\n",
    "y_reshaped = y.unsqueeze(0).unsqueeze(1)\n",
    "print(x_reshaped.shape)\n",
    "print(y_reshaped.shape)\n",
    "z = x_reshaped * y_reshaped\n",
    "print(\"after mul: \", z.size())\n",
    "z = torch.sum(z, dim=-1)\n",
    "print(\"after sum: \", z.size())\n",
    "# z = z.permute(b,h,l,r)\n",
    "print(z.size())\n",
    "z.allclose(ein_z)\n",
    "# x.matmul(y)\n",
    "\n",
    "# x_reshaped = x.reshape(b * h, l, d)\n",
    "# y_reshaped = y.reshape(l, d, r)\n",
    "\n",
    "# # Matrix multiplication\n",
    "# z_reshaped = torch.matmul(x_reshaped, y_reshaped)\n",
    "# print(z_reshaped.size())\n",
    "# # Reshape z\n",
    "# z = z_reshaped.reshape(b, h, l ,r)\n",
    "\n",
    "\n",
    "# z.allclose(ein_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative positional encoding: https://jaketae.github.io/study/relative-positional-encoding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a unsqueeze size:  torch.Size([2, 8, 1, 3, 3])\n",
      "b unsqueeze size:  torch.Size([2, 1, 4, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 8, 3, 3)\n",
    "b = torch.rand(2, 4, 3, 3)\n",
    "ans = torch.matmul(a.unsqueeze(2), b.unsqueeze(1))\n",
    "\n",
    "print(\"a unsqueeze size: \", a.unsqueeze(2).size())\n",
    "print(\"b unsqueeze size: \", b.unsqueeze(1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3.],\n",
       "         [6., 6.]],\n",
       "\n",
       "        [[3., 3.],\n",
       "         [6., 6.]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[[1,1],[2,2]], [[1,1],[2,2]]])\n",
    "print(x.size())\n",
    "torch.matmul(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  torch.Size([2, 2, 2, 2])\n",
      "y size:  torch.Size([2, 3, 2])\n",
      "x:  tensor([[[[ 1.,  2.],\n",
      "          [ 3.,  4.]],\n",
      "\n",
      "         [[ 5.,  6.],\n",
      "          [ 7.,  8.]]],\n",
      "\n",
      "\n",
      "        [[[ 9., 10.],\n",
      "          [11., 12.]],\n",
      "\n",
      "         [[13., 14.],\n",
      "          [15., 16.]]]])\n",
      "y:  tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 1.],\n",
      "         [0., 0.]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [2, 2] but got: [2, 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([[[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m],[\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m]], [[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m],[\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m]]])\n\u001b[0;32m     12\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([[[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m]], [[\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m]]])\n\u001b[1;32m---> 13\u001b[0m c \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(a,b)\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(c\u001b[39m.\u001b[39msize())\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(c)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [2, 2] but got: [2, 1]."
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[[[1,2],[3,4]],[[5,6],[7,8]]],[[[9,10],[11,12]],[[13,14],[15,16]]]]) \n",
    "y = torch.Tensor([[[0,0],[0,0],[0,0]],[[0,0],[0,1],[0,0]]])\n",
    "print(\"x size: \", x.size())\n",
    "print(\"y size: \", y.size())\n",
    "\n",
    "print(\"x: \", x)\n",
    "print(\"y: \", y)\n",
    "\n",
    "torch.einsum(\"bhld, lrd -> blhr\", x, y)\n",
    "\n",
    "a = torch.Tensor([[[1,2],[3,4]], [[1,2],[3,4]]])\n",
    "b = torch.Tensor([[[1,2,3], [1,2,3]], [[1,2,4], [1,2,3]]])\n",
    "c = torch.matmul(a,b)\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code originated from here: https://towardsdatascience.com/deep-dive-into-the-code-of-bert-model-9f618472353e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "class BertSelfAttention(nn.Module): \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_attention_heads = 6, \n",
    "        hidden_size = 768,   \n",
    "        dropout = 0.1,       \n",
    "        position_embedding_type = \"absolute\", \n",
    "\n",
    "    ): \n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states: torch.Tensor, \n",
    "        attention_mask: Optional[torch.FloatTensor] = None, \n",
    "        head_mask: Optional[torch.FloatTensor] = None, \n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, \n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]: \n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        is_cross_attention = encoder_hidden_states is not None \n",
    "\n",
    "        if is_cross_attention and past_key_value is not None: \n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention: \n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None: \n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else: \n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        \n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertTokenizer(): \n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_file, \n",
    "        do_lower_case = True, \n",
    "        do_basic_tokenize= True,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\", \n",
    "        sep_token=\"[SEP]\", \n",
    "        pad_token=\"[PAD]\", \n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\", \n",
    "        tokenize_chinese_chars=True, \n",
    "        strip_accents=None,\n",
    "        **kwargs\n",
    "    ): \n",
    "        super().__init__(\n",
    "            do_lower_case=do_lower_case, \n",
    "            do_basic_tokenize=do_basic_tokenize, \n",
    "            never_split=never_split, \n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token, \n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            tokenize_chinese_chars=tokenize_chinese_chars,\n",
    "            strip_accents=strip_accents,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(vocab_file): \n",
    "            raise ValueError(\n",
    "                \"cant find file\"\n",
    "            )\n",
    "\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in whitespace_tokenize(text): \n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word: \n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue \n",
    "        \n",
    "            is_bad = False\n",
    "            start = 0 \n",
    "            sub_tokens = []\n",
    "            while start < len(chars): \n",
    "                end = len(chars)\n",
    "                cur_substr = None \n",
    "                while start < end: \n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0: \n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab: \n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1 \n",
    "\n",
    "                if cur_substr is None: \n",
    "                    is_bad = True \n",
    "                    break \n",
    "\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end \n",
    "\n",
    "            if is_bad: \n",
    "                output_tokens.append(self.unk_token)\n",
    "            else: \n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1622,   131,  5837,  1197,  1775,  2050,   137, 20049,  1306,\n",
       "           119, 15276,  1181,   119,  5048,  1358,   113,  1187,   112,   188,\n",
       "          1139,  1645,   114, 12859, 16811,   131,   160, 11612,  1942,  1610,\n",
       "          1110,  1142,   106,   136,   151,  2227,  1643,   118,  3799,  1158,\n",
       "           118, 15919,   131,   187,  7409,  1495,   119, 20049,  1306,   119,\n",
       "         15276,  1181,   119,  5048,  1358,  6534,   131,  1239,  1104,  4624,\n",
       "           117,  1531,  1670, 12058,   131,  1405,   146,  1108,  6123,  1191,\n",
       "          2256,  1149,  1175,  1180,  4035,  4568,  1424,  1143,  1113,  1142,\n",
       "          1610,   146,  1486,  1103,  1168,  1285,   119,  1135,  1108,   170,\n",
       "           123,   118,  1442,  2865,  1610,   117,  1350,  1106,  1129,  1121,\n",
       "          1103,  1523, 22891,   120,  1346, 19025,   119,  1135,  1108,  1270,\n",
       "           170, 19217,  2836,   119,  1109,  3581,  1127,  1541,  1353,   119,\n",
       "          1130,  1901,   117,  1103,  1524, 26035,  1108,  2767,  1121,  1103,\n",
       "          1832,  1104,  1103,  1404,   119,  1188,  1110,  1155,   146,  1221,\n",
       "           119,  1409,  2256,  1169,  1587,  3263,   170,  2235,  1271,   117,\n",
       "          2395,   188, 25392,  1116,   117,  1201,  1104,  1707,   117,  1187,\n",
       "          1142,  1610,  1110,  1189,   117,  1607,   117,  1137,  3451, 23992,\n",
       "          1128,  1138,  1113,  1142, 19657,  1183,  1702,  1610,   117,  4268,\n",
       "           174,   118,  6346,   119,  5749,   117,   118, 15393,   118,   118,\n",
       "           118,   118,  1814,  1106,  1128,  1118,  1240,  4532,  3180,  1197,\n",
       "          1775,  2050,   118,   118,   118,   118,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1622,   131,  2564,  4786,  1186,   137,  3079,  1320,   119,\n",
       "           190,   119, 13445,  1633,   119,  5048,  1358,   113,  6173, 23209,\n",
       "          1186,   114, 12859, 16811,   131,   156,  2240, 20957, 24619,   118,\n",
       "          3788,  7268, 15463, 12917,  1616,   131,  3788,  1840,  1111,   156,\n",
       "          2240,  4705,  3756,  7443, 15573,   131,   156,  2240,   117, 18383,\n",
       "           117,  4705,   117, 12764,  8554,   118,   146,   119,   141,   119,\n",
       "           131,  5963,  2254,   119,   122,  4426,  1964, 14467,  1580, 11607,\n",
       "          2249,  1665,  1495,  1116,  6534,   131,  1239,  1104,  1994, 12058,\n",
       "           131,  1429,   151, 15681,  2101,   118,  3799,  1158,   118, 15919,\n",
       "           131,  3079,  1320,   119,   190,   119, 13445,  1633,   119,  5048,\n",
       "          1358,   138,  4652,  1295,  1104, 11313, 11191,  1150,  9554,  1147,\n",
       "           156,  2240,  4705,   184,  1116, 23694,  2772,  1138,  3416,  1147,\n",
       "          5758,  1111,  1142,  9590,   119,  4203,  3952,   170,  4094,  3802,\n",
       "         18954,  1240,  5758,  1114,  1103,  7791,   119,  3299,  2420, 12961,\n",
       "           117, 18701,  6317,  2420,   117,  5194,  1113,  4802,  1105, 16677,\n",
       "          1468,   117,  3208, 27004,   117,  2396,  1104,  7991,  1679,  1285,\n",
       "           117, 22593, 27643, 10437, 16354,  1114,  4645,  1105,   122,   119,\n",
       "           125,   182, 22593,  4184, 15108,  1132,  2108,  6792,   119,   146,\n",
       "          1209,  1129,  7584,  7317,  4404,  1107,  1103,  1397,  1160,  1552,\n",
       "           117,  1177,  4268,  5194,  1106,  1103,  2443,  3044,  2259,  1191,\n",
       "          1128,  1138,  1694,  1103,  4705, 12764,  1105,  3983,   112,   189,\n",
       "          3845,  1142,  9590,   119,  5749,   119,  6173, 23209,  1186,   133,\n",
       "          2564,  4786,  1186,   137,   190,   119, 13445,  1633,   119,  5048,\n",
       "          1358,   135,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1622,   131,   189, 17274,  1548,   137,   174,  1665,   119,\n",
       "           174,  1665,  1179,   119, 23609,  2956,  4175,   119,  5048,  1358,\n",
       "           113,  1819,   142, 12886,   114, 12859, 16811,   131,   153,  2064,\n",
       "          3243,   119,   119,   119,  6534,   131, 21537,  1239,  3939,  6701,\n",
       "          3998, 19806,   131,  1366,  1161, 12058,   131,  3164,  1218, 13918,\n",
       "           117,  1139, 23639,  4882,  1921,  1522,  1146,  1103,  7483,  1142,\n",
       "          5138,  1170,  2547,  1297,  1112,   170, 27445,  1377,  1236,  1171,\n",
       "          1107,  2210,   119,  1177,  5658,   117,   178,   112,   182,  1107,\n",
       "          1103,  2319,  1111,   170,  1207,  3395,   170,  2113, 10639,  1190,\n",
       "           178,  3005,  1106,  1129,   119,   119,   119,   178,   112,   182,\n",
       "          1702,  1154,  8184,  1146,   170,  1540,  6470,  7690,  1137,  2654,\n",
       "          7967,  1105,  1138,   170,  9670,  1104,  3243,  1115,   113, 16121,\n",
       "           114,  9994,  1169,  2590,   131,   115,  1674, 11183,  1221,  1251,\n",
       "          6786,  1113,  1165,  1103,  1397,  1668,  1104,  1540,  6470,  4784,\n",
       "          1116,  1132,  2637,   136,   178,   112,   173,  1767,  1103, 16183,\n",
       "          1665,  1108,  3155,  1106,  1294,  1126,  2845,  7008,   107,  1142,\n",
       "          2247,   107,  1133,  3983,   112,   189,  1767,  4169,  1113,  1122,\n",
       "           118,  1105,  1290,   178,  1274,   112,   189,  1138,  2469,  1106,\n",
       "         23639, 19094,  1377,   117,   178,  1108,  6123,  1191, 11183,  1149,\n",
       "          1175,  1125,  1167, 23992,   119,   119,   119,   115,  1144, 11183,\n",
       "          1767, 12287,  1164,  3945,  8949,  1106,  1103,  1540,  6470,  1413,\n",
       "          1176,  1103,  3200,  1103,  6862,   112,   188,  1198,  1355,  1194,\n",
       "          3055,   136,   115,  1184,   112,   188,  1103,  8351,  1104,  1103,\n",
       "          3934,  1113,  1103,  7967,   136,   178,  1180,  1930,  8046,   170,\n",
       "          7967,  1191,   178,  1400,  1103,  2908,  2107,  1830, 10437,  1897,\n",
       "          1190,  1103,  5356,   117,  1133,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import torch\n",
    "\n",
    "max_length = 256\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\", output_attentions=True, output_hidden_states=True, return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "config.max_position_embeddings = max_length\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "inputs_tests = tokenizer(newsgroups_train['data'][:3], truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "model = BertModel(config)\n",
    "\n",
    "inputs_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batch = 3\n",
    "shape_embs = (inputs_tests['input_ids'].shape) + (model.embeddings.word_embeddings.weight.shape[1], )\n",
    "w_embs_batch = torch.index_select(model.embeddings.word_embeddings.weight, 0, inputs_tests['input_ids'].reshape(1,-1).squeeze(0)).reshape(shape_embs)\n",
    "pos_embs_batch = torch.index_select(model.embeddings.position_embeddings.weight, 0, \n",
    "                                    torch.tensor(range(inputs_tests['input_ids'][1].size(0))).repeat(1, n_batch).squeeze(0)).reshape(shape_embs)\n",
    "type_embs_batch = torch.index_select(model.embeddings.token_type_embeddings.weight, 0, \n",
    "                                     inputs_tests['token_type_ids'].reshape(1,-1).squeeze(0)).reshape(shape_embs)\n",
    "batch_all_embs = w_embs_batch + pos_embs_batch + type_embs_batch\n",
    "batch_all_embs.shape # (batch_size, n_words, embedding dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings() \n",
    "        self.encoder = BertEncoder()\n",
    "        self.pooler = BertPooler() if add_pooling_layer else None \n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: Optional[torch.Tensor] = None, \n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None, \n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None, \n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None, \n",
    "        use_cache: Optional[bool] = None, \n",
    "        output_attentions: Optional[bool] = None, \n",
    "        output_hidden_states: Optional[bool] = None, \n",
    "        return_dict: Optional[bool] = None \n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions \n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.use_return_dict\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "inputs_tests = tokenizer(newsgroups_train['data'][:3], truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.index_select(model.embeddings.word_embeddings.weight, 0, inputs_tests['input_ids'][0]) \\\n",
    "+ torch.index_select(model.embeddings.position_embeddings.weight, 0, torch.tensor(range(inputs_tests['input_ids'][0].size(0))).long()) \\\n",
    "+ torch.index_select(moedl.embeddings.token_type_embeddings.weight, 0, inputs_tests[\"token_type_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world',\n",
       " 'yo',\n",
       " '.',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'j',\n",
       " '##y',\n",
       " 'j',\n",
       " '##ef',\n",
       " '##f']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"hello world yo. my name is jy jeff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tests['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups \n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "inputs_tests = tokenizer(newsgroups_train['data'][:3], truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "        'Hello, how are you? I am Romeo.\\n'\n",
    "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "        'Nice meet you too. How are you today?\\n'\n",
    "        'Great. My baseball team won the competition.\\n'\n",
    "        'Oh Congratulations, Juliet\\n'\n",
    "        'Thanks you Romeo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "for i, w in enumerate(word_list): \n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences: \n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 28, 10, 22, 14, 9, 26],\n",
       " [19, 26, 25, 21, 6, 15, 23, 18, 8, 22],\n",
       " [23, 8, 22, 13, 28, 10, 22, 16],\n",
       " [7, 25, 4, 12, 11, 20, 5],\n",
       " [24, 27, 15],\n",
       " [17, 22, 26]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30 \n",
    "batch_size = 6 \n",
    "max_pred = 5 \n",
    "n_layers = 6 \n",
    "n_heads = 12\n",
    "d_model = 768 \n",
    "d_ff = 768 \n",
    "d_k = d_v = 64 \n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(): \n",
    "    batch = []\n",
    "    positive = negative = 0 \n",
    "    while positive != batch_size/2 or negative != batch_size/2: \n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding() \n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        embed_weight = self.embedding.tok_embed.weight \n",
    "        n_vocab, n_dim = embed_weight.size() \n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "    \n",
    "    def forward(self, input_ids, segment_ids, masked_pos): \n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers: \n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0]))\n",
    "        logits_clsf = self.classifier(h_pooled)\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "\n",
    "        return logits_lm, logits_clsf \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
