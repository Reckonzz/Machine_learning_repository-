{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "Each of Q, K, V has shape (batch_size, seq_length, num_features)\n",
    "\n",
    "Components: \n",
    "- Scaled dot product attention \n",
    "- Attention Head \n",
    "- Multi head attention\n",
    "- Position Encoding \n",
    "- Feed forward \n",
    "- Residual \n",
    "- Transformer Encoder Layer\n",
    "- Transformer Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor \n",
    "import torch.nn.functional as f \n",
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor): \n",
    "    temp = query.bmm(key.transpose(1, 2)) # performs a batch matrix multiplication \n",
    "    scale = key.size(-1) ** 0.5 \n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(key: Tensor, query: Tensor, value: Tensor): \n",
    "    temp = query.bmm(key.transpose(1,2))\n",
    "    scale = key.size(-1) ** 0.5 \n",
    "    softmax = f.softmax(temp / scale)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clshe\\AppData\\Local\\Temp\\ipykernel_5280\\2446876834.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = f.softmax(temp / scale)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[66., 69., 72.],\n",
       "         [66., 69., 72.],\n",
       "         [66., 69., 72.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[[1,2,3], [4,5,6], [7,8,9]], [[10,11,12], [13,14,15], [16,17,18]], [[19,20,21], [22,23,24], [25,26,27]]]\n",
    "\n",
    "\n",
    "V = torch.tensor(data, dtype=torch.float)\n",
    "scaled_dot_product_attention(V, V, V)\n",
    "\n",
    "class AttentionHead(nn.Module): \n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int): \n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))\n",
    "    \n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int): \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor: \n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Network: a layer with skip connections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module): \n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int): \n",
    "        super.__init__()\n",
    "        self.keys = nn.Linear(dim_in, dim_k)\n",
    "        self.query = nn.Linear(dim_in, dim_q)\n",
    "        self.values = nn.Linear(dim_in, dim_k) \n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor: \n",
    "        return scaled_dot_product_attention(self.query(query), self.keys(key), self.values(value))\n",
    "    \n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int): \n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)  \n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor: \n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )\n",
    "\n",
    "def position_encoding(\n",
    "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"gpu\")\n",
    ") -> Tensor: \n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.flaot, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 **  (dim / dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n",
    "\n",
    "def feed_forward(dim_input: int=512, dim_feedforward: int=2048) -> nn.Module: \n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )\n",
    "    \n",
    "class Residual(nn.Module): \n",
    "    def  __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1): \n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer \n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor: \n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))\n",
    "    \n",
    "class TransformerEncoderLayer(nn.Module): \n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1\n",
    "    ): \n",
    "        super().__init__() \n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k), \n",
    "            dimension = dim_model, \n",
    "            dropout = dropout,\n",
    "        )\n",
    "\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward), \n",
    "            dimension=dim_model, \n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor: \n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)\n",
    "\n",
    "class TransformerEncoder(nn.Module): \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers: int = 6, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ): \n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(\n",
    "                    dim_model, num_heads, dim_feedforward, dropout\n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, src: Tensor) -> Tensor: \n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers: \n",
    "            src = layer(src)\n",
    "        return src"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "374d635f151312d2307cf40a2789094301ebf899bf64085654e3dce711a2b793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
